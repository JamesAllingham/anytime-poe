{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "from src.models import make_Reg_Ens_loss as make_loss\n",
    "import src.data\n",
    "from src.data import NumpyLoader\n",
    "from src.utils.training import setup_training, train_loop\n",
    "from experiments.configs.toy_reg_ens import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjamesallingham\u001b[0m (\u001b[33minvariance-learners\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'train_reg_ens.ipynb'\n",
    "# ^ W&B doesn't know how to handle VS Code notebooks.\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_fn = getattr(src.data, config.dataset_name)\n",
    "train_dataset, test_dataset, val_dataset = data_gen_fn(**config.dataset.to_dict())\n",
    "train_loader = NumpyLoader(train_dataset, config.batch_size)\n",
    "val_loader = NumpyLoader(val_dataset, config.batch_size)\n",
    "test_loader = NumpyLoader(test_dataset, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+------------+--------+-----------+--------+\n",
      "| Name                                        | Shape      | Size   | Mean      | Std    |\n",
      "+---------------------------------------------+------------+--------+-----------+--------+\n",
      "| batch_stats/nets_0/layer_0/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_0/layer_0/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_0/layer_1/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_0/layer_1/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_1/layer_0/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_1/layer_0/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_1/layer_1/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_1/layer_1/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_2/layer_0/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_2/layer_0/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_2/layer_1/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_2/layer_1/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_3/layer_0/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_3/layer_0/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_3/layer_1/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_3/layer_1/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_4/layer_0/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_4/layer_0/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| batch_stats/nets_4/layer_1/BatchNorm_0/mean | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| batch_stats/nets_4/layer_1/BatchNorm_0/var  | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/logscale                             | (1,)       | 1      | 0.0       | 0.0    |\n",
      "| params/nets_0/input_layer/bias              | (100,)     | 100    | -0.0436   | 0.605  |\n",
      "| params/nets_0/input_layer/kernel            | (1, 100)   | 100    | -0.115    | 0.52   |\n",
      "| params/nets_0/layer_0/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_0/layer_0/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_0/layer_0/Dense_0/bias          | (100,)     | 100    | -0.0726   | 0.619  |\n",
      "| params/nets_0/layer_0/Dense_0/kernel        | (100, 100) | 10,000 | 0.000147  | 0.0575 |\n",
      "| params/nets_0/layer_1/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_0/layer_1/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_0/layer_1/Dense_0/bias          | (100,)     | 100    | -0.0269   | 0.603  |\n",
      "| params/nets_0/layer_1/Dense_0/kernel        | (100, 100) | 10,000 | -0.000511 | 0.0575 |\n",
      "| params/nets_0/output_layer/bias             | (1,)       | 1      | -0.443    | 0.0    |\n",
      "| params/nets_0/output_layer/kernel           | (100, 1)   | 100    | 0.0033    | 0.0572 |\n",
      "| params/nets_1/input_layer/bias              | (100,)     | 100    | 0.106     | 0.544  |\n",
      "| params/nets_1/input_layer/kernel            | (1, 100)   | 100    | 0.0259    | 0.607  |\n",
      "| params/nets_1/layer_0/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_1/layer_0/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_1/layer_0/Dense_0/bias          | (100,)     | 100    | 0.0703    | 0.582  |\n",
      "| params/nets_1/layer_0/Dense_0/kernel        | (100, 100) | 10,000 | 0.000123  | 0.0575 |\n",
      "| params/nets_1/layer_1/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_1/layer_1/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_1/layer_1/Dense_0/bias          | (100,)     | 100    | -0.12     | 0.587  |\n",
      "| params/nets_1/layer_1/Dense_0/kernel        | (100, 100) | 10,000 | 0.000546  | 0.0575 |\n",
      "| params/nets_1/output_layer/bias             | (1,)       | 1      | 0.132     | 0.0    |\n",
      "| params/nets_1/output_layer/kernel           | (100, 1)   | 100    | 0.00177   | 0.0601 |\n",
      "| params/nets_2/input_layer/bias              | (100,)     | 100    | 0.0861    | 0.577  |\n",
      "| params/nets_2/input_layer/kernel            | (1, 100)   | 100    | -0.0367   | 0.549  |\n",
      "| params/nets_2/layer_0/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_2/layer_0/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_2/layer_0/Dense_0/bias          | (100,)     | 100    | 0.0773    | 0.578  |\n",
      "| params/nets_2/layer_0/Dense_0/kernel        | (100, 100) | 10,000 | -0.000543 | 0.0579 |\n",
      "| params/nets_2/layer_1/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_2/layer_1/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_2/layer_1/Dense_0/bias          | (100,)     | 100    | 0.112     | 0.576  |\n",
      "| params/nets_2/layer_1/Dense_0/kernel        | (100, 100) | 10,000 | -0.000392 | 0.0576 |\n",
      "| params/nets_2/output_layer/bias             | (1,)       | 1      | 0.882     | 0.0    |\n",
      "| params/nets_2/output_layer/kernel           | (100, 1)   | 100    | -0.0131   | 0.0575 |\n",
      "| params/nets_3/input_layer/bias              | (100,)     | 100    | -0.0255   | 0.609  |\n",
      "| params/nets_3/input_layer/kernel            | (1, 100)   | 100    | -0.0764   | 0.575  |\n",
      "| params/nets_3/layer_0/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_3/layer_0/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_3/layer_0/Dense_0/bias          | (100,)     | 100    | -0.0437   | 0.576  |\n",
      "| params/nets_3/layer_0/Dense_0/kernel        | (100, 100) | 10,000 | 0.000384  | 0.0577 |\n",
      "| params/nets_3/layer_1/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_3/layer_1/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_3/layer_1/Dense_0/bias          | (100,)     | 100    | 0.0771    | 0.589  |\n",
      "| params/nets_3/layer_1/Dense_0/kernel        | (100, 100) | 10,000 | 0.000774  | 0.0572 |\n",
      "| params/nets_3/output_layer/bias             | (1,)       | 1      | -0.0751   | 0.0    |\n",
      "| params/nets_3/output_layer/kernel           | (100, 1)   | 100    | -0.00499  | 0.0605 |\n",
      "| params/nets_4/input_layer/bias              | (100,)     | 100    | -0.0867   | 0.588  |\n",
      "| params/nets_4/input_layer/kernel            | (1, 100)   | 100    | -0.0351   | 0.561  |\n",
      "| params/nets_4/layer_0/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_4/layer_0/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_4/layer_0/Dense_0/bias          | (100,)     | 100    | 0.00748   | 0.558  |\n",
      "| params/nets_4/layer_0/Dense_0/kernel        | (100, 100) | 10,000 | -0.000229 | 0.0577 |\n",
      "| params/nets_4/layer_1/BatchNorm_0/bias      | (100,)     | 100    | 0.0       | 0.0    |\n",
      "| params/nets_4/layer_1/BatchNorm_0/scale     | (100,)     | 100    | 1.0       | 0.0    |\n",
      "| params/nets_4/layer_1/Dense_0/bias          | (100,)     | 100    | 0.0202    | 0.61   |\n",
      "| params/nets_4/layer_1/Dense_0/kernel        | (100, 100) | 10,000 | -0.000423 | 0.0581 |\n",
      "| params/nets_4/output_layer/bias             | (1,)       | 1      | -0.94     | 0.0    |\n",
      "| params/nets_4/output_layer/kernel           | (100, 1)   | 100    | 0.00162   | 0.0629 |\n",
      "| params/weights                              | (5,)       | 5      | 1.0       | 0.0    |\n",
      "+---------------------------------------------+------------+--------+-----------+--------+\n",
      "Total: 106,511\n"
     ]
    }
   ],
   "source": [
    "setup_rng, rng = random.split(rng)\n",
    "init_x = train_dataset[0][0]\n",
    "init_y = train_dataset[0][1]\n",
    "\n",
    "model, state = setup_training(config, setup_rng, init_x, init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdef46d344a4291b2a8c64bb52d8bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 - train loss: 2.10950, val_loss: 1.62705, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   2 - train loss: 1.47555, val_loss: 1.52929, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   3 - train loss: 1.32613, val_loss: 1.46609, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   4 - train loss: 1.34467, val_loss: 1.42987, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   5 - train loss: 1.29503, val_loss: 1.41331, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   6 - train loss: 1.21987, val_loss: 1.41003, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:   7 - train loss: 1.12627, val_loss: 1.42549, lr: 0.00010\n",
      "epoch:   8 - train loss: 1.11018, val_loss: 1.45219, lr: 0.00010\n",
      "epoch:   9 - train loss: 1.13372, val_loss: 1.47350, lr: 0.00010\n",
      "epoch:  10 - train loss: 1.14183, val_loss: 1.47784, lr: 0.00010\n",
      "epoch:  11 - train loss: 1.12735, val_loss: 1.46176, lr: 0.00010\n",
      "epoch:  12 - train loss: 1.09606, val_loss: 1.43070, lr: 0.00010\n",
      "epoch:  13 - train loss: 1.06787, val_loss: 1.39198, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  14 - train loss: 1.03326, val_loss: 1.35084, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  15 - train loss: 0.98521, val_loss: 1.31613, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  16 - train loss: 0.92140, val_loss: 1.29434, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  17 - train loss: 0.86415, val_loss: 1.29082, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  18 - train loss: 0.81359, val_loss: 1.30430, lr: 0.00010\n",
      "epoch:  19 - train loss: 0.78263, val_loss: 1.32580, lr: 0.00010\n",
      "epoch:  20 - train loss: 0.75952, val_loss: 1.37165, lr: 0.00010\n",
      "epoch:  21 - train loss: 0.74145, val_loss: 1.41054, lr: 0.00010\n",
      "epoch:  22 - train loss: 0.69339, val_loss: 1.43933, lr: 0.00010\n",
      "epoch:  23 - train loss: 0.65409, val_loss: 1.50320, lr: 0.00010\n",
      "epoch:  24 - train loss: 0.60787, val_loss: 1.46180, lr: 0.00010\n",
      "epoch:  25 - train loss: 0.54020, val_loss: 1.46763, lr: 0.00010\n",
      "epoch:  26 - train loss: 0.50163, val_loss: 1.53853, lr: 0.00010\n",
      "epoch:  27 - train loss: 0.49936, val_loss: 1.56028, lr: 0.00010\n",
      "epoch:  28 - train loss: 0.45969, val_loss: 1.61348, lr: 0.00010\n",
      "epoch:  29 - train loss: 0.42110, val_loss: 1.68423, lr: 0.00010\n",
      "epoch:  30 - train loss: 0.42437, val_loss: 1.64198, lr: 0.00010\n",
      "epoch:  31 - train loss: 0.53591, val_loss: 2.28956, lr: 0.00010\n",
      "epoch:  32 - train loss: 0.70299, val_loss: 2.36993, lr: 0.00010\n",
      "epoch:  33 - train loss: 0.80275, val_loss: 2.25724, lr: 0.00010\n",
      "epoch:  34 - train loss: 0.70838, val_loss: 1.86645, lr: 0.00010\n",
      "epoch:  35 - train loss: 1.05656, val_loss: 5.90524, lr: 0.00010\n",
      "epoch:  36 - train loss: 1.84670, val_loss: 2.29365, lr: 0.00010\n",
      "epoch:  37 - train loss: 0.97255, val_loss: 1.96537, lr: 0.00010\n",
      "epoch:  38 - train loss: 0.86224, val_loss: 3.05810, lr: 0.00010\n",
      "epoch:  39 - train loss: 0.87533, val_loss: 2.58287, lr: 0.00010\n",
      "epoch:  40 - train loss: 0.77529, val_loss: 1.90693, lr: 0.00010\n",
      "epoch:  41 - train loss: 0.74019, val_loss: 2.06827, lr: 0.00010\n",
      "epoch:  42 - train loss: 0.69449, val_loss: 2.13837, lr: 0.00010\n",
      "epoch:  43 - train loss: 0.63332, val_loss: 2.02151, lr: 0.00010\n",
      "epoch:  44 - train loss: 0.67285, val_loss: 1.45361, lr: 0.00010\n",
      "epoch:  45 - train loss: 0.59211, val_loss: 1.19668, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  46 - train loss: 0.60745, val_loss: 1.18129, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  47 - train loss: 0.60262, val_loss: 1.22436, lr: 0.00010\n",
      "epoch:  48 - train loss: 0.57599, val_loss: 1.16896, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  49 - train loss: 0.60907, val_loss: 1.04949, lr: 0.00010\n",
      "Best val_loss\n",
      "epoch:  50 - train loss: 0.59774, val_loss: 0.94763, lr: 0.00010\n",
      "Best val_loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3302cb0dcc348c1a29d4ca4539d1511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▅▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▂▃▂▇▃▃▃▂▂▂▂▂▂▂▂</td></tr><tr><td>val/loss</td><td>▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▃▃▃▃▃▃▃▃▅▆▅▄▅▄█▆▅▅▅▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>50</td></tr><tr><td>best_val_loss</td><td>0.94763</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.59774</td></tr><tr><td>val/loss</td><td>0.94763</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /homes/jua23/Git/anytime-poe/experiments/notebooks/wandb/offline-run-20220524_103405-336b4cg0<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20220524_103405-336b4cg0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = train_loop(\n",
    "    model, state, config, rng, make_loss, make_loss, train_loader, val_loader,\n",
    "    # test_loader,\n",
    "    wandb_kwargs={\n",
    "        'mode': 'offline',\n",
    "        # 'notes': '',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f1c3e72c33e9d599052a4f04c6971142100cc706e19ec52aa01eeb819b5a8e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('any-poe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
